{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Natural Language Technologies\n",
    "\n",
    "__Lecture 10, Transformers, BERT__\n",
    "\n",
    "__Nov 25, 2020__\n",
    "\n",
    "__Judit Ács__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Introduced in [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al., 2017\n",
    "\n",
    "[Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Recall that we used recurrent neural cells, specifaclly LSTMs to encode and decode sequences.\n",
    "\n",
    "LSTMs rely on their left and right history (horizontal arrows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/tikz/abstract_seq2seq.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it impossible to parallelize these steps.\n",
    "\n",
    "Transformers solve this problem by relying purely on attention instead of recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/encoder_with_tensors.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention assigns a weight to each element of the sequence.\n",
    "\n",
    "This weight is the _importance_ of the element, i.e. how much 'attention we should pay'.\n",
    "\n",
    "Self-attention means that the encoder attends to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is available in the [Tensor2tensor notebook in Google Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word order\n",
    "\n",
    "Without recurrence word order information is lost.\n",
    "\n",
    "Positional information is important:\n",
    "\n",
    "    John loves Mary.\n",
    "    Mary loves John.\n",
    "\n",
    "Transformers apply positional encoding:\n",
    "\n",
    "$$\n",
    "PE_{pos,2i} = sin(pos/10000^{2i/d_{\\text{model}}}), \\\\\n",
    "PE_{pos,2i+1} = cos(pos/10000^{2i/d_{\\text{model}}}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other components\n",
    "\n",
    "Transformers have a number of additional components summarized in this figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/transformer.png\")  # from Vaswani et al. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch support\n",
    "\n",
    "PyTorch has a `nn.Transformer` class and its encoder and decoder versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual embeddings\n",
    "\n",
    "(from Lecture 8) In GloVe and Word2vec representations, words have static representations, in other words, the same vector is assigned for every occurrence of the word.\n",
    "But words can have different meaning in different contexts, e.g. the word 'stick':\n",
    "\n",
    "1. Find some dry sticks and we'll make a campfire.\n",
    "2. Let's stick with glove embeddings.\n",
    "\n",
    "![elmo](http://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
    "\n",
    "_(Peters et. al., 2018 in the ELMo paper)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo\n",
    "\n",
    "**E**mbeddings from **L**anguage **Mo**dels\n",
    "\n",
    "Word representations are functions of the full sentences instead of the word alone.\n",
    "\n",
    "Two bidirectional LSTM layers are linearly combined.\n",
    "\n",
    "[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) by Peters et al., 2018, 5200 citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)\n",
    "by Devlin et al. 2018, 12500 citations\n",
    "\n",
    "[BERTology](https://huggingface.co/transformers/bertology.html) is the nickname for the growing amount of BERT-related research.\n",
    "\n",
    "Trained on two tasks:\n",
    "\n",
    "1. Masked language model:\n",
    "\n",
    "    1. 15% of the <s>tokens</s>wordpieces are selected at the beginning.\n",
    "    2. 80% of those are replaced with `[MASK]`,\n",
    "    3. 10% are replaced with a random token,\n",
    "    4. 10% are kept intact.\n",
    "    \n",
    "2. Next sentence prediction:\n",
    "    - Are sentences A and B consecutive sentences?\n",
    "    - Generate 50-50%.\n",
    "    - Binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer layers\n",
    "\n",
    "\n",
    "## Finetuning\n",
    "\n",
    "1. Take a trained BERT model.\n",
    "2. Add a small classification layer on top (typically a 2-layer MLP).\n",
    "3. Train BERT along with the classification layer on an annotated dataset.\n",
    "    - Much smaller than the data BERT was trained on\n",
    "\n",
    "Another option: freeze BERT and train the classification layer only.\n",
    "- Easier training regime.\n",
    "- Smaller memory footprint.\n",
    "- Worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_encoding_finetuning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT pretrained checkpoints\n",
    "\n",
    "### BERT-Base\n",
    "\n",
    "- 12 layers\n",
    "- 12 attention heads per layer\n",
    "- 768 hidden size\n",
    "- 110M parameters\n",
    "\n",
    "### BERT-Large\n",
    "\n",
    "- 24 layers\n",
    "- 16 attention heads per layer\n",
    "- 1024 hidden size\n",
    "- 340M parameters\n",
    "\n",
    "### Cased and uncased\n",
    "\n",
    "Uncased: everything is lowercased. Diacritics are removed.\n",
    "\n",
    "### Multilingual BERT - mBERT\n",
    "\n",
    "104 language version trained on the 100 largest Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT implementations\n",
    "\n",
    "[Original Tensorflow implementation](https://github.com/google-research/bert)\n",
    "\n",
    "[Huggingface Transformers](https://huggingface.co/transformers/)\n",
    "- PyTorch implementation originally for BERT-only\n",
    "- Now it supports dozens of other models\n",
    "- Hundreds of other model checkpoints from the community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tokenization\n",
    "\n",
    "## WordPiece tokenizer\n",
    "\n",
    "BERT's input **must** be\n",
    "A middle ground between word and character tokenization.\n",
    "\n",
    "Static vocabulary:\n",
    "- Byte-pair encoding: simple frequency-based tokenization method\n",
    "- Continuation symbols (\\#\\#symbol)\n",
    "- Special tokens: `[CLS]`, `[SEP]`, `[MASK]`, `[UNK]`\n",
    "- It tokenizes everything, falling back to characters and `[UNK]` if necessary\n",
    "\n",
    "`AutoTokenizer` is a factory class for pretrained tokenizers. ng id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"Русский\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cased** models keep diacritics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It character tokenizes Chinese and Japanese but doesn't know all the characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korean is missing from this version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mBERT tokenization\n",
    "\n",
    "104 languages, 1 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT\n",
    "\n",
    "## Using `BertModel` directly\n",
    "\n",
    "`AutoModel`\n",
    "- each pretrained checkpoint has a string id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "type(model), type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"There are black cats and black dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__call__` return a dictionary of BERT's encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"There are black cats and black dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used for multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"There are black cats and black dogs.\", \"There are two white cats.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need tensors as inputs for BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(\"There are black cats and black dogs.\", return_tensors='pt')\n",
    "encoded['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded, return_dict=True)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['last_hidden_state'].size(), output['pooler_output'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded, output_hidden_states=True, return_dict=True)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output['hidden_states']), output['hidden_states'][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT applications\n",
    "\n",
    "### Sequence classification\n",
    "\n",
    "Pretrained model for sentiment analysis.\n",
    "\n",
    "Base model: `distilbert-base-uncased`\n",
    "\n",
    "Finetuned on the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) or SST-2, a popular sentiment analysis dataset.\n",
    "\n",
    "Model id: `distilbert-base-uncased-finetuned-sst-2-english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "nlp(\"This is an amazing class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence tagging/labeling: Named entity recognition\n",
    "\n",
    "Base model: `bert-large-cased`\n",
    "\n",
    "Finetuned on [CoNLL-2003 NER](https://www.clips.uantwerpen.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(\"jupiter is a Planet that orbits around James the center of the Universe\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked language modeling\n",
    "\n",
    "Uses `distilroberta-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pred = nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\")\n",
    "pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]['token_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models\n",
    "\n",
    "## Pretrained models\n",
    "\n",
    "RoBERTa: identical model, larger training data, different training objective\n",
    "\n",
    "DistilBERT: smaller version of BERT. It was _distilled_ or compressed from BERT with a student-teacher setup.\n",
    "\n",
    "XLM-RoBERTa: multilingual version of RoBERTa\n",
    "\n",
    "Distil-mBERT: distilled multilingual BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community models\n",
    "\n",
    "[Over 1000 community contributions](https://huggingface.co/models)\n",
    "\n",
    "## huBERT\n",
    "\n",
    "The first and so far only Hungarian-only model\n",
    "\n",
    "BERT base, trained on Webcorpus 2.0, a version of CommonCrawl.\n",
    "\n",
    "Its tokenizer works much better for Hungarian than mBERT's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_tokenizer = AutoTokenizer.from_pretrained('SZTAKI-HLT/hubert-base-cc')\n",
    "# hubert = AutoModel.from_pretrained('SZTAKI-HLT/hubert-base-cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = (\"George Clooney Magyarországról szóló, az Orbán-kormányt kritizáló levelére miniszteri és \"\n",
    "        \"államtitkári szinten is reagált a magyar kormány.\")\n",
    "hubert_tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 text generation\n",
    "\n",
    "Causal language modeliing is when the $i^{th}$ token is modeled based on all the previous tokens as opposed to masked language modeling where both left and right context are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_generator(\"This is a serious issue we should address\", max_length=50, do_sample=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further information\n",
    "\n",
    "[Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- Famous blog post with a detailed gentle introduction to Transformers\n",
    "\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- A walkthrough of original Transformer paper with code and detailed illustration\n",
    "\n",
    "[Huggingface Transformers - Summary of tasks](https://huggingface.co/transformers/task_summary.html)\n",
    "\n",
    "[My blog post about mBERT's tokenizer](http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
